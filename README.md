# learning-to-bound
Implementation of the paper "Improving Variable Orderings of Approximate Decision Diagrams using Reinforcement Learning".

One challenge in the design of scalable algorithms is the efficient computation of tight optimization bounds. Decision diagrams (DDs) provide a novel and flexible mechanism for obtaining high-quality and flexible bounds. This work provides a generic framework based on deep reinforcement learning for improving the optimization bounds of dynamic programs, thanks to DD technology. This is done by learning appropriate variable orderings that are shown experimentally to tighten the bounds proven by restricted and relaxed DDs. A DD-based branch-and-bound using tight bounds generated by the trained models is presented and applied to the maximum independent set problem.

## Content of the repository

```bash
.
└── learning-DD/ # learning script
	├── graphnn/ # graph neural network library
	├── models/  
		└── misp-random/ # implementation for the maximum independent set
		      ├── config_run.sh  # training configuration file
		      ├── run_misp_training_random.sh  # training launcher
		      ├── code/
		      ├── results-local/ # models generated along with learning curves 
└── indepset/ # branch-and-bound script
	├── instances/ # maximum independent set instances
	├── sources/  
		├── src/ # branch-and-bound sources
		├── model/ # models used during branch-and-bound
		├── run_bb.sh # branch-and-bound launcher
```

### 1. Training a model

```shell
cd learning-DD/models/misp-random/
./run_misp_training_random.sh # training with config_run parameters
```

### 2. Solving the problem

```shell
cd indepset/sources/code/
./run_test.sh # example on a random graph with a pre-trained model
```
